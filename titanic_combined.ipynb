{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"final_train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(\"train.csv\")\n",
    "\n",
    "df['sex'] = df_2['Sex']\n",
    "df['embarked'] = df_2['Embarked']\n",
    "df['survived'] = df_2['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b93e05",
   "metadata": {},
   "source": [
    "Descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "print(df.describe())\n",
    "print(df.nunique())\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a182e",
   "metadata": {},
   "source": [
    "Cleaning column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace(r'[^\\w]', \"\", regex= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1379405e",
   "metadata": {},
   "source": [
    "Converting data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d91e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdadedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['survived'] = df['survived'].astype(bool)\n",
    "df['passengerid'] = df['passengerid'].astype('str')\n",
    "\n",
    "categories = ['pclass', 'sex', 'embarked']\n",
    "df[categories] = df[categories].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d767a",
   "metadata": {},
   "source": [
    "Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "df.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523d50c",
   "metadata": {},
   "source": [
    "The missing values in age column have been filled using prediction model based on:\n",
    "pclass, sibsp, parch, fare, embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e859ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embarked']=df['embarked'].fillna(df['embarked'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b651d",
   "metadata": {},
   "source": [
    "The missing values in cabin have been filled by extracting deck number from known cabin numbers (first letter of the number) and a model has been trained on it, using features like pclass, fare, sbsp, parch and embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02195a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92367785",
   "metadata": {},
   "source": [
    "No duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dabf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dbf7ef",
   "metadata": {},
   "source": [
    "Exporting the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('titanic_data_cleaning.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb3072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c7212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"titanic_data_cleaning.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac91cdc",
   "metadata": {},
   "source": [
    "Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b984f",
   "metadata": {},
   "source": [
    "1) numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380c0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "num_cols = [\"age\", \"fare\", \"sibsp\", \"parch\"]\n",
    "\n",
    "for col in num_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\n Summary Stats for {col}:\\n{df[col].describe()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90934b4c",
   "metadata": {},
   "source": [
    "categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd42c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"sex\", \"pclass\", \"embarked\", \"survived\", \"deck\"]\n",
    "\n",
    "for col in cat_cols:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(data=df, x=col)\n",
    "    plt.title(f\"Count of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nðŸ”¢ Value Counts for {col}:\\n{df[col].value_counts(dropna=False)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c3aa3",
   "metadata": {},
   "source": [
    "Bivariate and Multivariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23629ba5",
   "metadata": {},
   "source": [
    "1) Deck vs Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(data=df, x=\"deck\", hue=\"survived\")\n",
    "plt.title(\"Survival Count by Deck\")\n",
    "plt.xlabel(\"Deck\")\n",
    "plt.ylabel(\"Passenger Count\")\n",
    "plt.legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6b6aa",
   "metadata": {},
   "source": [
    "If we figure out what kind of passengers were staying at deck F we can uncover some insights, but it should be remembered that most of the data in deck is predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa70c55b",
   "metadata": {},
   "source": [
    "2) pclass vs survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c0723",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(data=df, x='pclass', hue= 'survived')\n",
    "plt.title('Survival Count by Passenger Class')\n",
    "plt.xlabel(\"Passenger Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Survived\", labels=[\"No\", \"Yes\"])\n",
    "plt.grid(True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c11c9",
   "metadata": {},
   "source": [
    "Most casualities belonged from the 3rd class, this makes sense because the first class might have been prioritized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508342f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99501e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"titanic_data_cleaning.csv\")\n",
    "df_2 = pd.read_csv('test_titanic_data_cleaning.csv')\n",
    "\n",
    "# df is the training dataset and df_2 is the testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d8a9e",
   "metadata": {},
   "source": [
    "Adding some new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
    "df_2['family_size'] = df_2['sibsp'] + df_2['parch'] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdba151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_alone'] = np.where(df['family_size'] == 1, 1, 0)\n",
    "\n",
    "df_2['is_alone'] = np.where(df_2['family_size'] == 1, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74950a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    df['age'] <= 14,\n",
    "    df['sex'] == 'female'\n",
    "]\n",
    "choices = [\n",
    "    'child',\n",
    "    'woman'\n",
    "]\n",
    "df['category'] = np.select(conditions, choices,  default='man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    df_2['age'] <= 14,\n",
    "    df_2['sex'] == 'female'\n",
    "]\n",
    "choices = [\n",
    "    'child',\n",
    "    'woman'\n",
    "]\n",
    "df_2['category'] = np.select(conditions, choices,  default='man')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f03e1f",
   "metadata": {},
   "source": [
    "Numerical Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b81a44",
   "metadata": {},
   "source": [
    "Starting with Fare, first checking outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='fare')\n",
    "plt.show()\n",
    "\n",
    "df['fare'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b06e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['fare'].quantile(0.25)\n",
    "Q3 = df['fare'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['fare'] < lower_bound) | (df['fare']> upper_bound )]\n",
    "print (\"num of outliers :\" , len(outliers))\n",
    "print (outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd4ada8",
   "metadata": {},
   "source": [
    "since we have some outliers and our std is higher than mean, we will use Robust scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "scaler = RobustScaler()\n",
    "\n",
    "df['fare_scaled'] = scaler.fit_transform(df[['fare']])\n",
    "\n",
    "\n",
    "joblib.dump(scaler, 'scaler.fare')\n",
    "\n",
    "df_2['fare_scaled'] = scaler.transform(df_2[['fare']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d850ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef453a86",
   "metadata": {},
   "source": [
    "Analyzing outliers for other numerical columns and applying scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a9b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = ['sibsp', 'parch', 'age']\n",
    "for col in num_cols:\n",
    "    plt.title(col)\n",
    "    sns.boxplot(data=df, x=col)\n",
    "    plt.show()\n",
    "    print(df[col].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6b2d7",
   "metadata": {},
   "source": [
    "Outlier handling for age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f119f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['age'].quantile(0.25)\n",
    "Q3 = df['age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['age'] < lower_bound) | (df['age']> upper_bound )]\n",
    "print (\"num of outliers :\" , len(outliers))\n",
    "print (outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = df['age'].clip(upper=60)\n",
    "print(df['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c87cc",
   "metadata": {},
   "source": [
    "checking for outliers again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1609ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('age_clipped')\n",
    "sns.boxplot(data=df, x=\"age\")\n",
    "plt.show()\n",
    "print(df['age'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2010b",
   "metadata": {},
   "source": [
    "Applying Z score standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df['age'] = scaler.fit_transform(df[['age']])\n",
    "\n",
    "\n",
    "joblib.dump(scaler, 'scaler.age')\n",
    "\n",
    "df_2['age'] = scaler.transform(df_2[['age']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5805e3",
   "metadata": {},
   "source": [
    "Now we analyze outliers for sibsp and parch, and handle them as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['sibsp'].quantile(0.25)\n",
    "Q3 = df['sibsp'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['sibsp'] < lower_bound) | (df['sibsp']> upper_bound )]\n",
    "print (\"num of outliers :\" , len(outliers))\n",
    "print (outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe458a4",
   "metadata": {},
   "source": [
    "We can't drop our outliers in sibsp because these are important and may give us information later, so we are going to use robust scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d886270",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "df['sibsp'] = scaler.fit_transform(df[['sibsp']])\n",
    "\n",
    "joblib.dump(scaler, 'scaler.sibsp')\n",
    "\n",
    "df_2['sibsp'] = scaler.transform(df_2[['sibsp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c126592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa9c313c",
   "metadata": {},
   "source": [
    "Now parch, first outliers then scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['parch'].quantile(0.25)\n",
    "Q3 = df['parch'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = df[(df['parch'] < lower_bound) | (df['parch']> upper_bound )]\n",
    "print (\"num of outliers :\" , len(outliers))\n",
    "print (outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c105355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "df['parch'] = scaler.fit_transform(df[['parch']])\n",
    "\n",
    "joblib.dump(scaler, 'scaler.parch')\n",
    "\n",
    "df_2['parch'] = scaler.transform(df_2[['parch']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d3cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2736ee5",
   "metadata": {},
   "source": [
    "Binary Encoding sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sex'] = df['sex'].map({'male':0, 'female': 1})\n",
    "df_2['sex'] = df_2['sex'].map({'male':0, 'female': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b26451c",
   "metadata": {},
   "source": [
    "Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c232e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['passengerid', 'name'], axis=1)\n",
    "\n",
    "df = df.drop(['ticket', 'cabin_filled'], axis=1)\n",
    "\n",
    "df_2 = df_2.drop(['passengerid', 'name'], axis=1)\n",
    "\n",
    "df_2 = df_2.drop(['ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab59a51",
   "metadata": {},
   "source": [
    "Finding correlation with survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa31769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Create a copy of your dataframe to avoid modifying the original\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Define your categorical columns\n",
    "categorical_cols = ['embarked', 'deck', 'category']\n",
    "\n",
    "# Apply label encoding to categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in df_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Handle missing values by filling them first (optional)\n",
    "        df_encoded[col] = df_encoded[col].fillna('Unknown')\n",
    "        df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "        label_encoders[col] = le  # Store encoder in case you need it later\n",
    "\n",
    "# 1. Start with domain knowledge features\n",
    "core_features = ['pclass', 'sex', 'age', 'fare_scaled', 'embarked']\n",
    "\n",
    "# 2. Add engineered features with good target correlation\n",
    "engineered_features = ['family_size', 'is_alone']\n",
    "\n",
    "# 3. Check correlations and remove redundant ones\n",
    "final_features = []\n",
    "correlation_with_target = df_encoded.corr()['survived'].abs()\n",
    "\n",
    "for feature in core_features + engineered_features:\n",
    "    if feature in correlation_with_target.index:\n",
    "        print(f\"{feature}: {correlation_with_target[feature]:.3f}\")\n",
    "        final_features.append(feature)\n",
    "\n",
    "print(\"Final feature set:\", final_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660ef556",
   "metadata": {},
   "source": [
    "Only selecting final features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f429de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your final feature list (including target for training set)\n",
    "final_features = ['pclass', 'sex', 'age', 'fare_scaled', 'embarked', 'sibsp', 'parch', 'family_size', 'is_alone']\n",
    "\n",
    "# For training set - keep features + target\n",
    "df_final = df[final_features + ['survived']].copy()\n",
    "\n",
    "# For test set - keep only features (no 'survived' column in test set)\n",
    "df_2_final = df_2[final_features].copy()\n",
    "\n",
    "print(\"Training set shape:\", df_final.shape)\n",
    "print(\"Test set shape:\", df_2_final.shape)\n",
    "print(\"Training columns:\", df_final.columns.tolist())\n",
    "print(\"Test columns:\", df_2_final.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e1676",
   "metadata": {},
   "source": [
    "Cateogorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features/target\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_cols = ['embarked']\n",
    "df_cleaned = df_final.copy()\n",
    "numerical_cols = [col for col in df_cleaned.columns if col not in categorical_cols + ['survived']]\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded = encoder.fit_transform(df_cleaned[categorical_cols])\n",
    "\n",
    "X = np.concatenate([df_cleaned[numerical_cols].values, encoded], axis=1)\n",
    "y = df_cleaned['survived'].values\n",
    "\n",
    "# Split into train/test for evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Run your comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e8f03",
   "metadata": {},
   "source": [
    "Now applying models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac62aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82ca965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def evaluate_models(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate selected classification models and compare their performance\n",
    "    \"\"\"\n",
    "    # Define models to test\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTesting {name}...\")\n",
    "        \n",
    "        try:\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            test_accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            results[name] = {\n",
    "                'CV_Mean': cv_scores.mean(),\n",
    "                'CV_Std': cv_scores.std(),\n",
    "                'Test_Accuracy': test_accuracy,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1_Score': f1,\n",
    "                'CV_Scores': cv_scores\n",
    "            }\n",
    "            \n",
    "            trained_models[name] = model\n",
    "            \n",
    "            print(f\"  Cross-Val Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "            print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
    "            print(f\"  F1-Score: {f1:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with {name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return results, trained_models\n",
    "\n",
    "def plot_model_comparison(results):\n",
    "    import pandas as pd\n",
    "    df_results = pd.DataFrame(results).T\n",
    "    \n",
    "    # Drop problematic column\n",
    "    if 'CV_Scores' in df_results.columns:\n",
    "        df_results = df_results.drop(columns=['CV_Scores'])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes[0,0].bar(df_results.index, df_results['CV_Mean'], \n",
    "                  yerr=df_results['CV_Std'], capsize=5, alpha=0.7)\n",
    "    axes[0,0].set_title('Cross-Validation Accuracy')\n",
    "    axes[0,0].set_ylabel('Accuracy')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[0,1].bar(df_results.index, df_results['Test_Accuracy'], \n",
    "                  color='orange', alpha=0.7)\n",
    "    axes[0,1].set_title('Test Set Accuracy')\n",
    "    axes[0,1].set_ylabel('Accuracy')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1,0].bar(df_results.index, df_results['F1_Score'], \n",
    "                  color='green', alpha=0.7)\n",
    "    axes[1,0].set_title('F1-Score Comparison')\n",
    "    axes[1,0].set_ylabel('F1-Score')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    metrics_df = df_results[['CV_Mean', 'Test_Accuracy', 'Precision', 'Recall', 'F1_Score']]\n",
    "\n",
    "    metrics_df = df_results[['CV_Mean', 'Test_Accuracy', 'Precision', 'Recall', 'F1_Score']]\n",
    "\n",
    "# Force convert to numeric - this will coerce invalid entries to NaN, then fill NaN with 0 or drop rows\n",
    "    metrics_df = metrics_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "    sns.heatmap(metrics_df.T, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "                ax=axes[1,1], cbar_kws={'label': 'Score'})\n",
    "    axes[1,1].set_title('All Metrics Heatmap')\n",
    "    axes[1,1].set_xlabel('Models')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_top_models(results, top_n=3):\n",
    "    \"\"\"\n",
    "    Identify and return top performing models\n",
    "    \"\"\"\n",
    "    sorted_models = sorted(results.items(), \n",
    "                          key=lambda x: x[1]['Test_Accuracy'], \n",
    "                          reverse=True)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"TOP {top_n} PERFORMING MODELS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for i, (name, metrics) in enumerate(sorted_models[:top_n], 1):\n",
    "        print(f\"\\n{i}. {name}\")\n",
    "        print(f\"   Test Accuracy: {metrics['Test_Accuracy']:.4f}\")\n",
    "        print(f\"   CV Accuracy: {metrics['CV_Mean']:.4f} (+/- {metrics['CV_Std']*2:.4f})\")\n",
    "        print(f\"   F1-Score: {metrics['F1_Score']:.4f}\")\n",
    "        print(f\"   Precision: {metrics['Precision']:.4f}\")\n",
    "        print(f\"   Recall: {metrics['Recall']:.4f}\")\n",
    "    \n",
    "    return [model[0] for model in sorted_models[:top_n]]\n",
    "\n",
    "def detailed_model_analysis(model_name, model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Provide detailed analysis for a specific model\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DETAILED ANALYSIS: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Not Survived', 'Survived'],\n",
    "                yticklabels=['Not Survived', 'Survived'])\n",
    "    plt.title(f'Confusion Matrix: {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        feature_names = [f'Feature_{i}' for i in range(len(importances))]\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=importance_df.head(10), x='Importance', y='Feature')\n",
    "        plt.title(f'Feature Importance: {model_name}')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(importance_df.head(10))\n",
    "\n",
    "def create_ensemble_model(top_models, trained_models, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Create an ensemble model using top performing models\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"CREATING ENSEMBLE MODEL\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    ensemble_estimators = [(name, trained_models[name]) for name in top_models[:3]]\n",
    "    \n",
    "    ensemble_model = VotingClassifier(\n",
    "        estimators=ensemble_estimators,\n",
    "        voting='soft'  # Use probabilities for voting\n",
    "    )\n",
    "    \n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Ensemble created with models: {[name for name, _ in ensemble_estimators]}\")\n",
    "    \n",
    "    return ensemble_model\n",
    "\n",
    "def run_model_comparison(X_train, y_train, X_test, y_test, feature_names=None):\n",
    "    \"\"\"\n",
    "    Run complete model comparison analysis\n",
    "    \"\"\"\n",
    "    print(\"Starting Model Comparison Analysis...\")\n",
    "    print(f\"Training set size: {X_train.shape}\")\n",
    "    print(f\"Test set size: {X_test.shape}\")\n",
    "    \n",
    "    results, trained_models = evaluate_models(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    plot_model_comparison(results)\n",
    "    \n",
    "    top_models = get_top_models(results, top_n=3)\n",
    "    \n",
    "    best_model_name = top_models[0]\n",
    "    best_model = trained_models[best_model_name]\n",
    "    detailed_model_analysis(best_model_name, best_model, X_test, y_test)\n",
    "    \n",
    "    ensemble_model = create_ensemble_model(top_models, trained_models, X_train, y_train)\n",
    "    \n",
    "    ensemble_pred = ensemble_model.predict(X_test)\n",
    "    ensemble_accuracy = accuracy_score(y_test, ensemble_pred)\n",
    "    ensemble_f1 = f1_score(y_test, ensemble_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"ENSEMBLE MODEL PERFORMANCE\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
    "    print(f\"Ensemble F1-Score: {ensemble_f1:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL RECOMMENDATIONS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best Single Model: {best_model_name}\")\n",
    "    print(f\"Best Single Model Accuracy: {results[best_model_name]['Test_Accuracy']:.4f}\")\n",
    "    print(f\"Ensemble Model Accuracy: {ensemble_accuracy:.4f}\")\n",
    "    \n",
    "    if ensemble_accuracy > results[best_model_name]['Test_Accuracy']:\n",
    "        print(\"âœ“ Recommendation: Use the Ensemble Model\")\n",
    "        recommended_model = ensemble_model\n",
    "    else:\n",
    "        print(\"âœ“ Recommendation: Use the Best Single Model\")\n",
    "        recommended_model = best_model\n",
    "    \n",
    "    return results, recommended_model, ensemble_model\n",
    "\n",
    "# Usage example:\n",
    "# results, best_model, ensemble_model = run_model_comparison(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7398247",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, best_model, ensemble_model = run_model_comparison(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7249770c",
   "metadata": {},
   "source": [
    "Making actual predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77df4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb557834",
   "metadata": {},
   "source": [
    "Categorical encoding for df_2_final which is the testing dataset with the final features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e4061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "categorical_cols = ['embarked']\n",
    "\n",
    "\n",
    "\n",
    "numerical_cols = [col for col in df_final.columns if col not in categorical_cols]\n",
    "numerical_cols_test = [col for col in df_2_final.columns if col not in categorical_cols]\n",
    "\n",
    "\n",
    "\n",
    "# Encode categorical\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_train_encoded = encoder.fit_transform(df_final[categorical_cols])\n",
    "\n",
    "# Save the encoder\n",
    "joblib.dump(encoder, 'encoder_titanic.pkl')\n",
    "\n",
    "X_test_encoded = encoder.transform(df_2_final[categorical_cols])\n",
    "\n",
    "\n",
    "# Combine features\n",
    "x_train = np.concatenate([\n",
    "    df_final[numerical_cols].values,\n",
    "    X_train_encoded\n",
    "], axis=1)\n",
    "\n",
    "x_test = np.concatenate([\n",
    "    df_2_final[numerical_cols_test].values,\n",
    "    X_test_encoded\n",
    "], axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c889820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8735990",
   "metadata": {},
   "source": [
    "best_model is the model chosen in models comparison, it is trained on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_model.predict(x_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_model, 'titanic_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668fab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# making a new data frame with passenger id and survived or not column\n",
    "\n",
    "df_original_test = pd.read_csv('test.csv')\n",
    "                               \n",
    "p_id = df_original_test['PassengerId']\n",
    "survived = y_test_pred\n",
    "\n",
    "new_df = pd.DataFrame({\n",
    "    'id': p_id,\n",
    "    'survived': survived\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "new_df.to_csv('test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b57abcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
